---
title: Machine Learning - Multiple Linear Regression
date: 2024-5-7 22:00:00 +/-TTTT
categories: [ML, Supervised Machine Learning]
tags: [Machine Learning, ML, Multiple Linear Regression]     # TAG names should always be lowercase

toc: true
toc_sticky: true
math: true  
use_math: true
mermaid: true

---

[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fepheria.github.io&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=views&edge_flat=false)](https://hits.seeyoufarm.com)

---

<br>

> 해당 포스트는 Andrew Ng 교수님의  [**Machine Learning Specialization**](https://www.coursera.org/specializations/machine-learning-introduction) 특화 과정에 대한 정리 내용을 참고하였습니다.

<br>
<br>

## 목차
---


<br>
<br>

## Vectorization - 벡터화
---

- Vectorization 을 사용하여 학습 알고리즘을 구현할 때, 코드를 더 간결하게 작성가능하고 실행 속도도 향상된다. 또한 현대적인 수치선형대수학 라이브러리를 사용 가능하며 GPU 하드웨어를 가속화 하기 위한 코드를 작성하기도 한다.

<br>

#### Parameters and features

- Vectorziation 을 적용하지 않은 방법으로 계산 해보자.

$$
\begin{align}
\vec{w} = [w_1 \quad w_2 \quad w_3] \\
\\
b \; \mbox{is a number} \\
\\
\vec{x} = [x_1 \quad x_2 \quad x_3]
\end{align}
$$

```python
w = np.array([1.0, 2.5, -3.3])
b = 4
x = np.array([10,20,30])
```

<br>

#### Without Vectorization 1

- 모델의 예측

$$
\large{f_{\vec{w}, b} (\vec{x}) = w_1x_1 + w_2x_2 + w_3x_3 + b}
$$

```python
f = w[0] * x[0] +
    w[1] * x[1] +
    w[2] * x[2] + b
```

- 이런 방법은 $n = 100000$ 까지 가면 매우 비효율적임..

<br>

#### Without Vectorization 2

- 따라서 그나마 효율적인 for 문을 돌려보면..

$$ 
\large{f_{\vec{w}, b} (\vec{x}) = \sum\limits_{j = 1}^{n} w_jx_j + b}
$$

```python
f = 0
for j in range(0, n):
    f = f + w[j] * x[j]
f = f + b
```

<br>

#### Vectorization

- Numpy 라이브러리를 사용하여 Vectorization 을 해보면 다음과 같다.

$$
\large{f_{\vec{w}, b} (\vec{x}) = \vec{w} \cdot \vec{x} + b}
$$

```python
f = np.dot(w,x) + b
```

- np.dot 함수는 컴퓨터의 병렬 하드웨어를 사용할 수 있다. 이는 CPU, GPU 를 사용하더라도 동일하다. 당연하지만 for 루프나 순차적 계산보다 훨씬 효율적임

<br>
<br>

## Vectorization 비교
---

- 실제로 위의 Without Vectorzation 과 Vectorization 의 코드를 절차적으로 파악하며 비교해보자.

<br>

![Desktop View](/assets/img/post/ml/ml05_01.png){: : width="300" .normal }    

- 직렬적으로 처리

<br>

![Desktop View](/assets/img/post/ml/ml05_02.png){: : width="400" .normal }    

- 병렬적으로 처리
- 벡터화 처리된 코드가 훨씬 빠르고 대규모 데이터 세트에서 알고리즘을 실행하거나 학습시킬 때 효율적이다.

<br>

- Gradient Descent 예시

![Desktop View](/assets/img/post/ml/ml05_03.png){: : width="800" .normal }    

- Vectorization 을 사용하면 선형 회귀를 훨씬 더 효율적으로 구현할 수 있다.

<br>
<br>

## Python Numpy 실습
---

- Numpy 의 기본 데이터 구조는 동일한 유형 (`dtype`)의 요소를 포함하는 인덱싱 가능한 n-dimensional *array*이다. 여기서 'dimension'이라는 용어가 오버로드 되었음을 알 수 있다. 여기서 벡터는 1차원 배열이다. (n,) 을 의미.

<br>

#### Vector Creation

- 벡터 생성 방법

{% include test4.html %}


- 선형 회귀를 확장하여 여러개의 input feature 를 처리하는 방법을 알아보자. 또한 벡터화 vectorization, 특징 스케일링 feature scaling, 특징 엔지니어링 feature engineering, 다항식 회귀 polynomial regression 등 모델의 훈련과 성능을 개선하기 위한 방법들이 존재한다.

## 다중 선형 회귀

기존 선형회귀에서는 단 한개의 feature 가지고 진행했었음

집의 크기 뿐만아니라, 방의 수 층수 집의 연차 등을 통해 가격을 예측하는데 더 많은 정보를 얻을 수 있다.

x_1,x_2,x_3.x_4 변수를 사용하여 네 가지 feature 를 표현해보자

$ x_j = j^{th} feature$

n = number of features

x->(i) features of ith training example (벡터임)

x->(2) 의 경우 = [1416, 3, 2, 40]  벡터!! row vector..!!


x_j^(i) = value of feature j in ith training example
x_3^(2) = 2



모델은 어떤지 살펴보자

기존 f_w,b (x) = wx + b

다변량 선형회귀 f_w,b (x) = w1x1 + w2x2 + w3x3 + w4x4 ... wnxn + b

w-> = [w1, w2, ... wn] 벡터 column vector

x-> = [x1 x2  ... xn] row vector

row x column





Vectorization 백터화를 사용하면 코드가 짧아지고 훨씬 효율적으로 실행됨

numpy 쓰라 이말이야!

np.dot
내적연산





